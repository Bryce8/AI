{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdb73e9d",
   "metadata": {},
   "source": [
    "# RFP: Maze Solvers\n",
    "\n",
    "## Project Overview\n",
    "You are invited to submit a proposal that answers the following question:\n",
    "\n",
    "### What path will your elf take?\n",
    "\n",
    "*Please submit your proposal by **2/11/25 at 11:59 PM**.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b6a60",
   "metadata": {},
   "source": [
    "## Required Proposal Components\n",
    "\n",
    "### 1. Data Description\n",
    "In the code cell below, use [Gymnasium](https://gymnasium.farama.org/) to set up a [Frozen Lake maze](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) for your project. When you are done with the set up, describe the reward system you plan on using.\n",
    "\n",
    "*Note, a level 5 maze is at least 10 x 10 cells large and contains at least five lake cells.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8744bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"gymnasium[toy-text]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "34314a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "\n",
    "# Define a custom 10x10 Frozen Lake map with an elf navigating\n",
    "lake_map = [\n",
    "    \"SFFFFFFFFF\",\n",
    "    \"FFFFFFFFFH\",\n",
    "    \"FFFHFFFFFF\",\n",
    "    \"FFFFFFHFFF\",\n",
    "    \"FFHFFFFFFF\",\n",
    "    \"FFFFFFFHFF\",\n",
    "    \"HFFFFFFHFF\",\n",
    "    \"FFFHFFFFFH\",\n",
    "    \"FFFFFFHFFF\",\n",
    "    \"FFFFFFFGFH\"  # Goal 'G' at (9,7)\n",
    "]\n",
    "\n",
    "# Create the Frozen Lake environment with visualization\n",
    "env = gym.make(\"FrozenLake-v1\", desc=lake_map, is_slippery=True, render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset environment to initialize display\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Initialize Q-table\n",
    "state_size = env.observation_space.n\n",
    "action_size = env.action_space.n\n",
    "q_table = np.zeros((state_size, action_size))\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 1000\n",
    "\n",
    "# Custom reward system\n",
    "reward_mapping = {\"goal\": 1, \"hole\": -1, \"step\": -0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e6b56c1-58bc-43dc-b0fd-dc4e64715878",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ed9f9",
   "metadata": {},
   "source": [
    "#### The reward system helps the AI learn the best path through the Frozen Lake maze. It gets +1 point for reaching the goal, -1 point for falling into a hole, and -0.01 points for every step to prevent wandering. This teaches the AI to be efficient, avoid risks, and find the shortest, safest route to the goal. Over time, it learns to make smarter moves and navigate the maze effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f80b3f",
   "metadata": {},
   "source": [
    "### 2. Training Your Model\n",
    "In the cell seen below, write the code you need to train a Q-Learning model. Display your final Q-table once you are done training your model.\n",
    "\n",
    "*Note, level 5 work uses only the standard Python library and Pandas to train your Q-Learning model. A level 4 uses external libraries like Baseline3.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "73e62305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Q-table:\n",
      "        Left      Down     Right        Up\n",
      "0  -0.294671 -0.292833 -0.293955 -0.293950\n",
      "1  -0.285957 -0.285889 -0.284421 -0.284965\n",
      "2  -0.275067 -0.274232 -0.274103 -0.274269\n",
      "3  -0.252797 -0.253413 -0.248511 -0.252426\n",
      "4  -0.228002 -0.229408 -0.224211 -0.226945\n",
      "..       ...       ...       ...       ...\n",
      "95  0.057365  0.081949  0.541531  0.003183\n",
      "96  0.061957  0.829164 -0.100900  0.100357\n",
      "97  0.000000  0.000000  0.000000  0.000000\n",
      "98  0.562657  0.000000  0.000000  0.000000\n",
      "99  0.000000  0.000000  0.000000  0.000000\n",
      "\n",
      "[100 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Q-learning algorithm\n",
    "for episode in range(num_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])  # Exploit best known action\n",
    "        \n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        # Adjust reward based on custom reward mapping\n",
    "        if terminated and reward == 1:\n",
    "            reward = reward_mapping[\"goal\"]\n",
    "        elif terminated:\n",
    "            reward = reward_mapping[\"hole\"]\n",
    "        else:\n",
    "            reward = reward_mapping[\"step\"]\n",
    "        \n",
    "        # Update Q-value\n",
    "        q_table[state, action] = q_table[state, action] + learning_rate * (\n",
    "            reward + discount_factor * np.max(q_table[new_state, :]) - q_table[state, action]\n",
    "        )\n",
    "        \n",
    "        state = new_state\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)  # Decay exploration rate\n",
    "\n",
    "# Display final Q-table\n",
    "q_table_df = pd.DataFrame(q_table, columns=[\"Left\", \"Down\", \"Right\", \"Up\"])\n",
    "print(\"Final Q-table:\")\n",
    "print(q_table_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b912364",
   "metadata": {},
   "source": [
    "### 3. Testing Your Model\n",
    "In the cell seen below, write the code you need to test your Q-Learning model for **1000 episodes**. It is important to test your model for 1000 episodes so that we are all able to compare our results.\n",
    "\n",
    "*Note, level 5 testing uses both a success rate and an average steps taken metric to evaluate your model. Level 4 uses one or the other.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e30cf1dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success Rate: 54.30%\n",
      "Average Steps Taken: 72.72\n"
     ]
    }
   ],
   "source": [
    "# Testing the trained model\n",
    "test_episodes = 1000\n",
    "success_count = 0\n",
    "step_counts = []\n",
    "\n",
    "for _ in range(test_episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state, :])  # Always exploit the best known action\n",
    "        new_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        steps += 1\n",
    "        state = new_state\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if terminated and reward == 1:\n",
    "            success_count += 1\n",
    "    \n",
    "    step_counts.append(steps)\n",
    "\n",
    "success_rate = success_count / test_episodes\n",
    "average_steps = np.mean(step_counts)\n",
    "\n",
    "print(f\"Success Rate: {success_rate * 100:.2f}%\")\n",
    "print(f\"Average Steps Taken: {average_steps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1fc365-018b-4e1c-930c-c50aab2c878c",
   "metadata": {},
   "source": [
    "### 4. Final Answer\n",
    "In the first cell below, describe the path your elf takes to get to the gift. *Note, a level 5 answer includes a gif of the path your elf takes in order to reach the gift.*\n",
    "\n",
    "In the second cell seen below, describe how well your Q-Learning model performed. Make sure that you explicitly name the **learning rate**, **the discount factor**, and the **reward system** that you used when training your final model. *Note, a level 5 description describes the model's performance using two types of quantitative evidence.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9bd2dcae-6fb3-4a16-9b5b-d56e59d09ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIF saved as elf_path.gif\n"
     ]
    }
   ],
   "source": [
    "# Simulate and render the fastest path\n",
    "def simulate_fastest_path(env, q_table):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    frames = []\n",
    "    \n",
    "    while not done:\n",
    "        frame = env.render()\n",
    "        frames.append(frame)  # Ensure frames are correctly stored as RGB arrays\n",
    "        action = np.argmax(q_table[state, :])  # Always exploit the best known action\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "    \n",
    "    env.close()\n",
    "    return np.array(frames, dtype=np.uint8)  # Ensure frames are proper image format\n",
    "# Save the path as a gif\n",
    "frames = simulate_fastest_path(env, q_table)\n",
    "\n",
    "# Save the path as a gif\n",
    "def save_gif(frames, filename=\"elf_path.gif\"):\n",
    "    with imageio.get_writer(filename, mode='I', duration=0.3) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "    print(f\"GIF saved as {filename}\")\n",
    "\n",
    "save_gif(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508eeab7-cee4-41f5-8fc6-44255e7852a9",
   "metadata": {},
   "source": [
    "<!-- import os\n",
    "from IPython.display import Image, display  # Import Image and display functions\n",
    "\n",
    "if os.path.exists(\"elf_path.gif\"):\n",
    "    os.remove(\"elf_path.gif\")\n",
    "\n",
    "save_gif(frames)\n",
    "\n",
    "# Display the updated GIF\n",
    "display(Image(filename=\"elf_path.gif\"))\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b5f0e-5e58-4b37-ae88-dc59aa7ec2d4",
   "metadata": {},
   "source": [
    "![Dancing Elf](elf_path.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fdae15",
   "metadata": {},
   "source": [
    "#### My elf travels down to the right, then down to the left, before going back up, to the right, down and to the right and down until the elf lands next to the gift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8f66ed",
   "metadata": {},
   "source": [
    "#### The Q-Learning model was successful 54.30% of the time, meaning it reached the goal a little more than half the time. On average, it took 72.72 steps to finish, which suggests the model hasn’t fully figured out the fastest way to reach the goal. To improve, it might need more iterations of training, and change the learning rate to be lower. That way, its main goal is to get to the gift; exploration rate should also decrease so that we exploit more than explore. Change the initial Q value to have more favorable initial training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56c756-a282-45bd-8003-7a4cf44b71b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
