{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9919c268",
   "metadata": {},
   "source": [
    "# Choose a Data Set\n",
    "\n",
    "Create your own dataset by scraping one of the following websites *(level 5)*:\n",
    "- [Wikipedia](https://www.wikipedia.org/)\n",
    "- [OpenLibrary](https://openlibrary.org/)\n",
    "\n",
    "**OR** \n",
    "\n",
    "Use data gathered from one of the following APIs *(level 4)*: \n",
    "- [TMDB](https://developer.themoviedb.org/reference/intro/getting-started)\n",
    "- [College Scorecard](https://collegescorecard.ed.gov/data/api-documentation/)\n",
    "\n",
    "**OR** \n",
    "\n",
    "Pick a JSON dataset *(level 3)*:\n",
    "- [Food/Restaurant Data](https://drive.google.com/drive/folders/1V94S6WpclvQmbnW88KVMD4EruryA1oma?usp=drive_link)\n",
    "- [Fashion Data](https://drive.google.com/drive/folders/1V8SbFjtRRW8WVf3xBzg0gzLjOtMhHea_?usp=drive_link)\n",
    "\n",
    "**OR** \n",
    "\n",
    "Pick a CSV dataset *(level 2)*:\n",
    "- [LA Parking Tickets](https://drive.google.com/drive/folders/1vaOfwMi6QmZEGsXr8VM0ulPGzvTTBCgm?usp=drive_link)\n",
    "- [Hotels](https://drive.google.com/drive/folders/1IpVFxgwBJvJHKoOuBsk6WK2qYqFYP4hi?usp=drive_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae147ea6",
   "metadata": {},
   "source": [
    "# My Question\n",
    "## What is the probability that a random Wikipedia article will contain the following climate-related terms: \"climate,\" \"emissions,\" \"energy,\" \"global,\" and \"warming\"? What is the probability that all of the climate-related terms from above are on the same page of a Wikipedia article?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149f324f",
   "metadata": {},
   "source": [
    "# My Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86d9935-4ab1-4a52-8a56-0f8993cccec1",
   "metadata": {},
   "source": [
    "## Data collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b228559e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation of Wikipedia pages...\n",
      "Valid Wikipedia pages: 159 out of 171\n",
      "Starting scraping of valid pages...\n",
      "Scraped content from 159 valid Wikipedia pages.\n",
      "All data saved to 'valid_combined_wikipedia_data.txt'.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to validate if a Wikipedia page exists\n",
    "def validate_wikipedia_page(keyword):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{keyword.replace(' ', '_')}\"\n",
    "    response = requests.get(url)\n",
    "    # Check if the page exists (HTTP status 200) and is not a placeholder or redirect\n",
    "    if response.status_code == 200 and 'Wikipedia does not have an article' not in response.text:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# Function to scrape valid Wikipedia pages\n",
    "def scrape_wikipedia(keyword):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{keyword.replace(' ', '_')}\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    # Extract paragraphs\n",
    "    paragraphs = soup.find_all('p')\n",
    "    text = ' '.join([p.get_text() for p in paragraphs])\n",
    "    return text\n",
    "\n",
    "# Function to remove non-existent pages\n",
    "def filter_valid_pages(page_list):\n",
    "    return [keyword for keyword in page_list if validate_wikipedia_page(keyword)]\n",
    "\n",
    "# Known valid Wikipedia pages (500 topics total)\n",
    "known_pages = [\n",
    "    # First 400 known pages (climate-related and general topics)\n",
    "    \"Artificial_intelligence\", \"Machine_learning\", \"Climate_change\", \"Global_warming\", \"Carbon_emissions\",\n",
    "    \"Sustainability\", \"Renewable_energy\", \"Greenhouse_gas\", \"Deforestation\", \"Sea_level_rise\",\n",
    "    \"Solar_power\", \"Wind_power\", \"Geothermal_energy\", \"Hydropower\", \"Paris_Agreement\",\n",
    "    \"COP26\", \"Biodiversity\", \"Methane_emissions\", \"Net_zero\", \"Electric_vehicles\",\n",
    "    \"Energy_transition\", \"Smart_cities\", \"Urban_sustainability\", \"Environmental_policy\",\n",
    "    \"Sustainable_agriculture\", \"Afforestation\", \"Climate_resilience\", \"Carbon_capture\",\n",
    "    \"Zero_emissions\", \"Ocean_acidification\", \"Fossil_fuels\", \"Climate_justice\", \"Environmental_activism\",\n",
    "    \"Nature-based_solutions\", \"Ecosystem_restoration\", \"Wildlife_conservation\", \"Marine_conservation\",\n",
    "    \"Python_(programming_language)\", \"Albert_Einstein\", \"World_War_II\", \"History_of_the_Internet\",\n",
    "    \"Quantum_mechanics\", \"Black_hole\", \"Isaac_Newton\", \"Blockchain\", \"Cryptocurrency\",\n",
    "    \"Data_science\", \"Big_data\", \"Computer_science\", \"Operating_system\", \"Linux\",\n",
    "    \"Windows_10\", \"MacOS\", \"Video_games\", \"Artificial_neural_networks\", \"Genetic_algorithms\",\n",
    "    \"Game_theory\", \"Probability_theory\", \"Statistics\", \"Linear_algebra\", \"Calculus\",\n",
    "    \"Machine_translation\", \"Natural_language_processing\", \"OpenAI\", \"Elon_Musk\", \"Nikola_Tesla\",\n",
    "    \"Photosynthesis\", \"Cell_(biology)\", \"Mitochondria\", \"Protein_synthesis\", \"Human_brain\",\n",
    "    \"Charles_Darwin\", \"Evolutionary_biology\", \"Galileo_Galilei\", \"The_Hubble_Space_Telescope\", \"Einstein's_theory_of_relativity\",\n",
    "    \"Atoms\", \"Periodic_table\", \"Chemical_reactions\", \"Newton's_laws_of_motion\", \"Thermodynamics\",\n",
    "    \"Climate_modeling\", \"Kepler's_laws_of_planetary_motion\", \"Space_travel\", \"Mars_exploration\", \"The_Moon\",\n",
    "    \"Wormholes\", \"Stephen_Hawking\", \"Quantum_entanglement\", \"Superconductors\", \"Artificial_general_intelligence\",\n",
    "    \"Deep_learning\", \"Reinforcement_learning\", \"Robotic_process_automation\", \"Algorithm\", \"Binary_search\",\n",
    "    \"Sorting_algorithms\", \"Data_structures\", \"Graph_theory\", \"Operating_system_kernels\", \"Computer_networks\",\n",
    "    \"Distributed_computing\", \"Quantum_cryptography\", \"Blockchain_applications\", \"Digital_currency\", \"Augmented_reality_glasses\",\n",
    "    \"Neural_network_architectures\", \"Genome_editing\", \"CRISPR_Cas9\", \"Vaccines\", \"Epidemiology\",\n",
    "    \"Coronavirus\", \"Virology\", \"Parasitology\", \"Immunology\", \"Antibiotics\",\n",
    "    \"Photosynthesis\", \"Carbon_cycle\", \"Tropical_forests\", \"Deserts\", \"Volcanoes\",\n",
    "    \"Earthquakes\", \"Geothermal_heat\", \"Fossilization\", \"Dinosaurs\", \"Human_ancestors\",\n",
    "    \"Paleontology\", \"History_of_Rome\", \"The_Greek_Philosophers\", \"World_War_I\", \"Medieval_Europe\",\n",
    "    \"Colonialism\", \"Industrialization\", \"French_Revolution\", \"Renaissance_art\", \"The_Harlem_Renaissance\",\n",
    "    \"William_Shakespeare\", \"The_Great_Gatsby\", \"To_Kill_a_Mockingbird\", \"Pride_and_Prejudice\", \"Moby-Dick\",\n",
    "    \"Sherlock_Holmes\", \"Harry_Potter\", \"The_Lord_of_the_Rings\", \"The_Marvel_Cinematic_Universe\", \"Game_of_Thrones\",\n",
    "    \"Netflix_originals\", \"YouTube_creators\", \"TikTok_trends\", \"The_Rock\", \"Beyonce\",\n",
    "    \"The_Grammys\", \"The_Oscars\", \"The_Nobel_Prize\", \"Mount_Everest\", \"The_Amazon_Rainforest\",\n",
    "    \"The_Great_Barrier_Reef\", \"Space_race\", \"Apollo_program\", \"Hubble_telescope\", \"Electric_planes\",\n",
    "    \"Self-driving_cars\", \"Hydrogen_economy\", \"Wind_turbines\", \"Solar_cells\", \"Clean_energy_innovations\",\n",
    "    \"Sustainable_materials\", \"Eco-friendly_inventions\", \"Biodegradable_plastics\", \"Circular_economy\",\n",
    "    \"Water_desalination\"\n",
    "\n",
    "]\n",
    "\n",
    "# Combine all steps into one main function\n",
    "def main():\n",
    "    print(\"Starting validation of Wikipedia pages...\")\n",
    "    # Validate all pages\n",
    "    valid_keywords = filter_valid_pages(known_pages)\n",
    "    print(f\"Valid Wikipedia pages: {len(valid_keywords)} out of {len(known_pages)}\")\n",
    "    \n",
    "    print(\"Starting scraping of valid pages...\")\n",
    "    # Scrape content from valid pages\n",
    "    texts = [scrape_wikipedia(keyword) for keyword in valid_keywords]\n",
    "    \n",
    "    # Combine all text into one dataset\n",
    "    combined_text = ' '.join(texts)\n",
    "    \n",
    "    # Save the valid content to a file\n",
    "    with open(\"valid_combined_wikipedia_data.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(combined_text)\n",
    "    \n",
    "    print(f\"Scraped content from {len(valid_keywords)} valid Wikipedia pages.\")\n",
    "    print(\"All data saved to 'valid_combined_wikipedia_data.txt'.\")\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d630e0e5-5d74-4f92-89e2-749ffcf151c8",
   "metadata": {},
   "source": [
    "# Analasys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b591101-8a38-466f-ace9-ff8223158575",
   "metadata": {},
   "source": [
    "## Theoretical Probability with the Multiplication Rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "680060e5-7cde-404f-9cd2-a212e23e7e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(climate and change): 5.983421609751169e-06\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "total_words = sum(word_counts.values())\n",
    "\n",
    "\n",
    "# Tokenize and count word frequencies\n",
    "words = combined_text.lower().split()  # Split the combined text into words\n",
    "word_counts = Counter(words)  # Create a Counter object to store word frequencies\n",
    "\n",
    "p_climate = word_counts[\"climate\"] / total_words\n",
    "p_change = word_counts[\"change\"] / total_words\n",
    "\n",
    "# Probability of both appearing in a single sentence\n",
    "p_both = p_climate * p_change\n",
    "print(\"P(climate and change):\", p_both)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc8bcb",
   "metadata": {},
   "source": [
    "***The probability of both \"climate\" and \"change\" appearing in the same sentence on a Wikipedia page is approximately 5.98 * 10^-6, showing their co-occurrence is very rare. This suggests that Wikipedia articles use varied language to discuss climate-related topics, rather than repeating the same phrases.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8e59cb-6eb3-4e8a-a9b6-17b6fad71303",
   "metadata": {},
   "source": [
    "## Expected Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "69c67502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected values for key words: {'climate': 2462.0, 'emissions': 1281.0, 'energy': 2720.0, 'global': 1511.0, 'warming': 463.99999999999994}\n"
     ]
    }
   ],
   "source": [
    "key_words = [\"climate\", \"emissions\", \"energy\", \"global\", \"warming\"]\n",
    "expected_values = {word: (word_counts.get(word, 0) / total_words) * total_words for word in key_words}\n",
    "\n",
    "print(\"Expected values for key words:\", expected_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93cfe1",
   "metadata": {},
   "source": [
    "***The expected values show \"climate\" appears about 2,462 times, \"emissions\" 1,281 times, \"energy\" 2,720 times, \"global\" 1,511 times, and \"warming\" 464 times. This highlights their importance in climate-related Wikipedia articles, focusing on energy, emissions, and global challenges.***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d391bf19-c50a-45a2-a619-2fcfc99ad1a4",
   "metadata": {},
   "source": [
    "<!-- ## Probability Distribution -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94e3fabf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Probability distribution\n",
    "# probabilities = [count / total_words for count in word_counts.values()]\n",
    "# plt.hist(probabilities, bins=10, edgecolor='black')\n",
    "# plt.title(\"Probability Distribution of Word Frequencies\")\n",
    "# plt.xlabel(\"Probability\")\n",
    "# plt.ylabel(\"Frequency\")\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7d2f97-cc1f-4a6f-829f-fd686355d591",
   "metadata": {},
   "source": [
    "<!-- ***The chart shows a skewed distribution where most words have very low probabilities, and only a few words are used frequently. This means the articles include many different words, but a small number of key terms dominate the content.*** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e0b499-e278-48bd-b883-90c02a764d0a",
   "metadata": {},
   "source": [
    "# My conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127575e-4456-4134-86a0-b42fd024cf30",
   "metadata": {},
   "source": [
    "***The first question, \"What is the probability that a random Wikipedia article will contain the following climate-related terms: 'climate,' etc.?\" is answered by the analysis of the expected value. Keywords like \"climate,\" \"emissions,\" and \"energy\" appear frequently, with expected occurrences of 2,462, 1,281, and 2,720 times respectively, showing they are key topics in Wikipedia articles about climate. \r\n",
    "The second question, \"What is the probability that all of the climate-related terms from above are in the same page of a Wikipedia article?\" is addressed by the probability analysis. The probability of \"climate\" and \"change\" appearing together in the same sentence is very low, calculated as 5.98 * 10^-6. This indicates that while the topics are widely discussed, their co-occurrence in the same sentence is rare, reflecting Wikipedia's use of diverse language to cover various aspects of climate issues comprehensively.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae68bb-8ef6-496e-93c5-c4f699a79dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
